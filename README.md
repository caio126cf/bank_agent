# ğŸ¤– Banco Ãgil â€“ Assistente Virtual

Este projeto implementa um assistente virtual bancÃ¡rio capaz de autenticar usuÃ¡rios, realizar solicitaÃ§Ãµes de aumento de limite, conduzir entrevistas de crÃ©dito e consultar cotaÃ§Ãµes de cÃ¢mbio.  
Ele utiliza **LangChain**, **LangGraph**,**CSV** e **uv**.

Este repositÃ³rio Ã© um **fork** do projeto open source original:  
https://github.com/luizomf/react_agent_langgraph_course

---

## Link testando o Assistente

https://youtu.be/EMrnbtZROLM

## ğŸ“Œ VisÃ£o Geral do Projeto

O objetivo do projeto Ã© criar um **agente bancÃ¡rio inteligente**, seguindo regras fortes de atendimento e tomando decisÃµes com base nas interaÃ§Ãµes do cliente.

Funcionalidades principais:

- AutenticaÃ§Ã£o de clientes via CPF + data de nascimento  
- SolicitaÃ§Ã£o de aumento de limite  
- Entrevista de crÃ©dito quando o aumento Ã© negado  
- CotaÃ§Ã£o de cÃ¢mbio  
- Encerramento de atendimento  
- ExecuÃ§Ã£o totalmente guiada por ferramentas (tools) integradas ao LLM

---

## ğŸ— Arquitetura do Sistema

A arquitetura combina:

### **1. LangGraph**
Usado para organizar o fluxo:

- `call_llm` â†’ chama o modelo  
- `tool_node` â†’ executa ferramentas solicitadas pelo LLM  
- `router` â†’ verifica se hÃ¡ tool_call e decide o prÃ³ximo nÃ³  

O fluxo Ã© cÃ­clico:

START â†’ call_llm â†’ (router) â†’ tool_node â†’ call_llm â†’ ... â†’ END


### **2. Prompt Engineering**
Grande parte do comportamento do agente vem do `SYSTEM_PROMPT` em `prompts.py`:

- Define fluxo de triagem  
- Regras de autenticaÃ§Ã£o  
- Fluxo de aumento de limite  
- Entrevista de crÃ©dito  
- CotaÃ§Ã£o de cÃ¢mbio  
- Comportamento empÃ¡tico  
- Regras de quando chamar tools  

Por enquanto, **a lÃ³gica do fluxo estÃ¡ no prompt** â€” ainda nÃ£o 100% determinÃ­stico via grafo.

### **3. Tools**
As ferramentas ficam em `tools.py` e realizam tarefas como:

- `authenticate_user`
- `solicitar_aumento_limite`
- `recalcular_score`
- `quote_currency`
- `encerrar_conversa`

Todas as tools acessam os CSVs dentro da pasta `db/`.

### **4. Base de Dados**
A pasta `db/` atua como um banco de dados simples:

db/
â”œâ”€â”€ score_limite.csv
â”œâ”€â”€ solicitacoes_aumento_limite.csv
â””â”€â”€ users.csv


### **5. Estado da Conversa**
A classe `State` define:

- HistÃ³rico de mensagens  
- Dados persistidos pelo fluxo  

Usado pelo LangGraph para coordenar a execuÃ§Ã£o.

---

## âš™ Funcionalidades Implementadas

- Saudar o usuÃ¡rio sempre na primeira interaÃ§Ã£o
- AutenticaÃ§Ã£o com atÃ© **3 tentativas**
- Bloqueio da conversa caso falhe as 3 tentativas
- SolicitaÃ§Ã£o de aumento de limite via tool
- Redirecionamento automÃ¡tico para entrevista de crÃ©dito se rejeitado
- Entrevista de crÃ©dito completa exigindo 5 dados
- Recalcular score e permitir definir novo limite
- CotaÃ§Ã£o de cÃ¢mbio (pede moedas se necessÃ¡rio)
- Encerrar atendimento de forma amigÃ¡vel
- Lidar com erros das tools de forma clara e educativa

---

## ğŸ§© Desafios Enfrentados

- **Tornar o fluxo determinÃ­stico com LangGraph**  
  O LangGraph exige rotas claras entre os nÃ³s, mas o comportamento do LLM Ã© dinÃ¢mico.  
  Foi difÃ­cil garantir previsibilidade 100% sÃ³ via grafo. 

  **SoluÃ§Ã£o atual:**  
  Centralizei a maior parte da lÃ³gica no prompt, planeando migrar para um fluxo mais determinÃ­stico no futuro.

- Manter validaÃ§Ã£o de dados (CPF, datas etc.) robusta
- Garantir que o LLM sÃ³ chame tools nos momentos corretos
- Controlar limites de tentativas sem perder o estado
- Organizar o loop `llm â†’ tool â†’ llm` sem criar loops infinitos

---

## ğŸ”§ Escolhas TÃ©cnicas & Justificativas

### **LangGraph**
Permite criar fluxos determinÃ­sticos e separar:
- processamento do LLM
- execuÃ§Ã£o de ferramentas
- controle do estado

Mesmo que ainda haja lÃ³gica no prompt, o grafo jÃ¡ torna o sistema mais robusto.

### **CSV como "banco de dados"**
Para um protÃ³tipo/junior backend, CSV Ã© simples e rÃ¡pido para testar lÃ³gica.

Futuramente: PostgreSQL ou SQLite.

### **LangChain + LLM com tools**
- Facilita chamar funÃ§Ãµes reais diretamente  
- Boa integraÃ§Ã£o para protÃ³tipos e automaÃ§Ãµes  

### **Prompt Engineering forte**
Controla o fluxo enquanto o grafo ainda estÃ¡ simples.

---

## â–¶ Tutorial de ExecuÃ§Ã£o

## ğŸš€ InstalaÃ§Ã£o do Ollama e configuraÃ§Ã£o do modelo gpt-oss:20b (Windows)

Este projeto roda **totalmente offline**, utilizando o **Ollama** como servidor local de modelos. NÃ£o exclusivamente, no arquivo utils.py podemos consumir uma LLM hospedada em cloud.
Siga os passos abaixo para instalar o Ollama no Windows e baixar o modelo necessÃ¡rio.

---

### ğŸ”§ 1. Instale o Ollama (Windows)

Baixe o instalador oficial:

ğŸ‘‰ https://ollama.com/download

ApÃ³s instalar, reinicie o terminal.  

Para verificar se estÃ¡ funcionando:

```bash
ollama --version
```
### ğŸ”§ 2. Baixe o modelo gpt-oss:20b

Execute no terminal:

```bash
ollama pull gpt-oss:20b
```

Isso farÃ¡ o download completo do modelo (pode demorar).

### ğŸ”§ 3. Certifique-se de que o Ollama estÃ¡ rodando no Windows
```bash
ollama serve
```

### ğŸ”§ 4. Instalar e sincronizar uv

```bash
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```
Verifique se estÃ¡ instalado:

```bash
uv --version
```

Para sincronizar (dependÃªncias), na raiz do projeto rode:

```bash
uv sync
```

### ğŸ”§ 5. Rodar projeto.

Com o uv instalado e sincronizado, rode:

```bash
uv run src/main.py
```