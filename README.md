# ü§ñ Banco √Ågil ‚Äì Assistente Virtual



---

## Link testando o Assistente

O v√≠deo foi levemente editado, como rodei consumi o gpt

https://youtu.be/EMrnbtZROLM

## üìå Vis√£o Geral do Projeto

Este projeto implementa um assistente virtual banc√°rio capaz de autenticar usu√°rios, realizar solicita√ß√µes de aumento de limite, conduzir entrevistas de cr√©dito e consultar cota√ß√µes de c√¢mbio.  
Ele utiliza **LangChain**, **LangGraph**, **csv**, **uv** e **Ollama**.

Este reposit√≥rio √© um **fork** do projeto open source original:  
https://github.com/luizomf/react_agent_langgraph_course

O objetivo do projeto √© criar um **agente banc√°rio inteligente**, seguindo regras fortes de atendimento e tomando decis√µes com base nas intera√ß√µes do cliente.

Funcionalidades principais:

- Autentica√ß√£o de clientes via CPF + data de nascimento  
- Solicita√ß√£o de aumento de limite  
- Entrevista de cr√©dito quando o aumento √© negado  
- Cota√ß√£o de c√¢mbio  
- Encerramento de atendimento  
- Execu√ß√£o totalmente guiada por ferramentas (tools) integradas ao LLM

---

## üèó Arquitetura do Sistema

A arquitetura combina:

### **1. LangGraph**
Usado para padronizar o fluxo, mas n√£o controlar as decis√µes:

- `call_llm` ‚Üí chama o modelo  
- `tool_node` ‚Üí executa ferramentas solicitadas pelo LLM  
- `router` ‚Üí verifica se h√° tool_call e encerrar caso n√£o tenha

O fluxo √© c√≠clico:

START ‚Üí call_llm ‚Üí (router) ‚Üí tool_node ‚Üí call_llm ‚Üí ... ‚Üí END

![Imagem de fluxo graph](assets\fluxo graph.png)

### **2. Prompt Engineering**
Grande parte do comportamento do agente vem do `SYSTEM_PROMPT` em `prompt.py` (arquivo singular):

- Define fluxo de triagem  
- Regras de autentica√ß√£o  
- Fluxo de aumento de limite  
- Entrevista de cr√©dito  
- Cota√ß√£o de c√¢mbio  
- Comportamento emp√°tico  
- Regras de quando chamar tools  

Por enquanto, **a l√≥gica do fluxo est√° no prompt** ‚Äî ainda n√£o 100% determin√≠stico via grafo.

### **3. Tools**
As ferramentas ficam em `tools.py` e realizam tarefas com:

- `triagem` (autentica√ß√£o de usu√°rios)
- `solicitar_aumento_limite`
- `recalcular_score`
- `quote_currency`
- `encerrar_conversa`

Todas as tools acessam os CSVs dentro da pasta `db/`.

### **4. Base de Dados**
A pasta `db/` atua como um banco de dados simples contendo score_limite.csv, solicitacoes_aumento_limite.csv e users.csv.


### **5. Threads e Mem√≥ria**

Ciclo de Vida de uma Thread

Cria√ß√£o:
`main.py`
config = RunnableConfig(configurable={"thread_id": 1})

Uso:
# Cada intera√ß√£o usa o mesmo thread_id
`main.py`
result = graph.invoke({"messages": current_loop_messages}, config=config)

Persist√™ncia:
O InMemorySaver mant√©m o estado na RAM.
Checkpoints s√£o salvos automaticamente ap√≥s cada invoke.
O hist√≥rico completo √© acumulado.

Finaliza√ß√£o:
Quando o programa termina, a mem√≥ria √© liberada.

---

## üß© Desafios Enfrentados

- **Tornar o fluxo determin√≠stico com LangGraph, lidar com Multiagentes**
  O LangGraph exige rotas claras entre os n√≥s, mas o comportamento do LLM √© din√¢mico.  
  Foi dif√≠cil garantir previsibilidade 100% s√≥ via grafo. 

  **Solu√ß√£o atual:**  
  Centralizei a maior parte da l√≥gica no prompt, planejando migrar para um fluxo mais determin√≠stico no futuro via Graph.py. Tamb√©m normalizar o idioma do c√≥digo completamente para ingl√™s.

- Manter valida√ß√£o de dados (CPF, datas etc.) robusta.
- Garantir que o LLM s√≥ chame tools nos momentos corretos.
- Controlar limites de tentativas sem perder o estado.
- Organizar o loop `llm ‚Üí tool ‚Üí llm` e permitir uma sa√≠da ao encerrar.

---

## üîß Escolhas T√©cnicas & Justificativas

### **LangChain e LangGraph**
Permite criar fluxos determin√≠sticos e separar:
- consumo de LLM em cloud e local.
- execu√ß√£o de ferramentas como agentes.
- controle do estado e mem√≥ria.
- lidar com multiagents.
- √© o framework mais utilizado atualmente e por isso optei por ele.

### **CSV como "banco de dados"**
Para um teste t√©cnico CSV √© simples, r√°pido e √∫til.

### **Prompt Engineering**
Controla o fluxo enquanto o grafo ainda est√° simples.

---

### Tutorial de Execu√ß√£o

### üöÄ Instala√ß√£o do Ollama e configura√ß√£o do modelo gpt-oss:20b (Windows)  

Este projeto roda **totalmente offline**, utilizando o **Ollama** como servidor local de modelos. N√£o exclusivamente, no arquivo utils.py podemos consumir uma LLM hospedada em cloud.
Siga os passos abaixo para instalar o Ollama no Windows e baixar o modelo necess√°rio.

---

### üîß 1. Instale o Ollama (Windows)

Baixe o instalador oficial:

üëâ https://ollama.com/download

Ap√≥s instalar, reinicie o terminal.  

Para verificar se est√° funcionando:

```bash
ollama --version
```
### üîß 2. Baixe o modelo gpt-oss:20b

Execute no terminal:

```bash
ollama pull gpt-oss:20b
```

Isso far√° o download completo do modelo (pode demorar).

### üîß 3. Certifique-se de que o Ollama est√° rodando no Windows
```bash
ollama serve
```

### üîß 4. Instalar e sincronizar uv

```bash
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```
Verifique se est√° instalado:

```bash
uv --version
```

Para sincronizar (depend√™ncias), na raiz do projeto rode:

```bash
uv sync
```

### üîß 5. Rodar projeto.

Com o uv instalado e sincronizado, rode:

```bash
uv run src/main.py
```